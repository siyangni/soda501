---
title: "SoDA501 Data Clean"
output: html_notebook
---

# Data Analysis


## Data Preperation

```{r}
# Load the dplyr package (using pacman)
library(pacman)
p_load(tidyverse)

# Read the CSV file into a data frame
df <- read.csv("/storage/home/szn5432/work/soda501/preprocessed/cleaned_data.csv")

# Change the label to factor
df$Nicotine12 <- as.factor(df$Nicotine12)
levels(df$Nicotine12) <- c("Negative", "Positive")

# Remove columns 'X.1' and 'X' (seems useless)
df <- df %>%
  select(-X.1, -X)

# View the first few rows of the data frame
head(df)
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Create training, validation, and test sets using slice
n <- nrow(df)
df_shuffled <- df %>% sample_n(n)  # Shuffle the entire dataframe

# 8-1-1 Split
train_end <- floor(0.8 * n)
valid_end <- floor(0.9 * n)

train_set <- df_shuffled %>% slice(1:train_end)
validation_set <- df_shuffled %>% slice((train_end + 1):valid_end)
test_set <- df_shuffled %>% slice((valid_end + 1):n)

# Check dimensions
dim(train_set)
dim(validation_set)
dim(test_set)
```



## Decision Tree Algorithm

We begin with the basic decision tree algorithm, without any ensembles, this is to set a baseline model performance for us to build prediction models.

First, setting up the training control
```{r}
# Load the caret package
p_load(caret)
p_load(rpart)

# Set up cross-validation
train_control <- trainControl(
  method = "cv",       # k-fold cross-validation
  number = 10,         # 5 folds to begin with
  verboseIter = TRUE,  # print training iterations
  classProbs = TRUE, # Classfication task
  savePredictions = "final",  # save predictions for each fold
  summaryFunction = twoClassSummary  # Use AUC and Accuracy

)

```

Define the tunegrid. **I checked why you have error "Error: The tuning parameter grid should have columns cp". This happens because caret only takes cp in the hyperparameter grid.**
```{r}
# Define the tuning grid
tune_grid <- expand.grid(
  cp = seq(0.01, 0.1, by = 0.001)
)
```

Model Training
```{r}
# Fit the model
# Train the decision tree model
model <- train(
  Nicotine12 ~ .,  # Predicting Nicotine12 using all other variables
  data = train_set,
  method = "rpart",  # Using rpart method for decision trees
  metric = "Accuracy",  # Metric to evaluate and select the best model
  maximize = TRUE,   # Specify to maximize the evaluation metric
  trControl = train_control,
  tuneGrid = tune_grid,
  na.action = na.omit,
  parms = list(split='information')
)

# Print model performance
print(model)
```


Testing model performance on the validation set

```{r}
# Predict on the validation set
predictions <- predict(model, newdata = validation_set)

# Calculate the confusion matrix
conf_matrix <- confusionMatrix(predictions, validation_set$Nicotine12)


# Print accuracy
accuracy <- conf_matrix$overall['Accuracy']
print(paste("Accuracy on the validation set is:", accuracy))

# Print additional metrics
print(paste("Precision:",  conf_matrix$byClass['Precision']))
print(paste("Recall (Sensitivity):", conf_matrix$byClass['Recall']))
print(paste("F1 Score:", conf_matrix$byClass['F1']))
```

Check the Variable Importance

```{r}

#Feature Importance
perm_importance <- varImp(model, scale = FALSE)

# Print the sorted importance scores
print(perm_importance)
```
